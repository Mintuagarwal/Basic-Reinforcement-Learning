{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is a values-based learning algorithm. Value based algorithms updates the value function based on an equation(particularly Bellman equation). Whereas the other type, policy-based estimates the value function with a greedy policy obtained from the last policy improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model to update the Q-values in the Q-table is given below where $\\alpha$ and $\\gamma$ are the learning rate and the discount factor 0 < $\\alpha$, $\\gamma$ < 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Qlearning_update.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Q-learning is a model-free algorithm, we use a enviroment from gym. Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. Here in our example we use a Car which has to reach a position located at the top of a mountain which the Car has to climb in order to complete the goal. At the goal, the reward is maximum and hence the most desired state. The Car is given an $action$ to which the environment responds by assigning a new state to the car and providing a $reward$ for the action and a boolean variable whether the goal was achieved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "#env.reset()\n",
    "\n",
    "alpha = 0.18\n",
    "discount = 0.95\n",
    "Episodes = 500 #goal to be actually reached efficiently by episode 2000 with the above parameters\n",
    "freq = 100\n",
    "\n",
    "discrete_os_size = [20] * len(env.observation_space.low) #using 20 blocks/windows to represent all possible states\n",
    "discrete_block_size = (env.observation_space.high - env.observation_space.low)/discrete_os_size\n",
    "\n",
    "q_values = np.random.normal(0, 2, size = (discrete_os_size + [env.action_space.n]))\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    dis_state = (state - env.observation_space.low)/discrete_block_size\n",
    "    dis_state = dis_state.astype(np.int)\n",
    "    return tuple(dis_state)\n",
    "\n",
    "#dis_state = get_discrete_state(env.reset())\n",
    "#print(q_values[dis_state].shape)\n",
    "\n",
    "for episode in range(Episodes + 1) :\n",
    "    done = False\n",
    "    dis_state = get_discrete_state(env.reset())\n",
    "    cnt = 0\n",
    "    if episode % freq == 0:\n",
    "        #print(q_values)\n",
    "        print(episode)\n",
    "    while not done:\n",
    "        #print(dis_state)\n",
    "        action = np.argmax(q_values[dis_state])\n",
    "        new_state, reward, done, valx = env.step(action)\n",
    "        temp_dis_state = get_discrete_state(new_state)\n",
    "        if episode % freq == 0 :\n",
    "            env.render()\n",
    "        if not done:\n",
    "            curr = q_values[dis_state + (action, )]\n",
    "            mx = np.max(q_values[temp_dis_state])\n",
    "            q_values[dis_state + (action, )] = (1-alpha)*curr + alpha*(reward + discount*mx)\n",
    "        elif new_state[0] >= env.goal_position :\n",
    "            q_values[dis_state + (action, )] = 0\n",
    "        dis_state = temp_dis_state\n",
    "        cnt += 1\n",
    "        if cnt > 1000 :\n",
    "            break\n",
    "        #print(cnt)\n",
    "                                                                 \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
